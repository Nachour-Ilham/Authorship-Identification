{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c8556e3-1b0b-479b-afe4-888afe5aa368",
   "metadata": {},
   "source": [
    "<h1 style=\"color:pink; text-align:center\">\n",
    "    <strong>Authorship Identification: Racine & Corneille</strong>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22aa783-70a4-408a-b384-992dbb12dcfb",
   "metadata": {},
   "source": [
    "### Create a SparkSession object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff13686-f4a4-454c-afb9-df8a5d50fe94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.extraListeners\", \"\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbef5be-40e0-42dc-8ed8-ffa976f7524b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11954cb-7a92-4604-9212-a5314778997c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#data analysis and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "#interacting with the operating system\n",
    "import os\n",
    "\n",
    "#extracting text and metadata from PDF files\n",
    "import pdfplumber\n",
    "\n",
    "#regular expression operations\n",
    "import re\n",
    "\n",
    "#natural language processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#download the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "#machine learning\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import LogisticRegression,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b1a1d-0d06-455b-a68e-6bb6a84fd07c",
   "metadata": {},
   "source": [
    "### Text Chunking Function\n",
    "\n",
    "takes a text and a chunk size as input and splits the text into chunks of the specified size. The function returns a list of text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a722983-6fa6-4a1e-97d3-32e446cef098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size):\n",
    "    \"\"\"Splits the text into chunks of a specified size.\n",
    "\n",
    "    Args:\n",
    "        text: The text to be split.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "            current_chunk += word + ' '\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = word + ' '\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7382fb0-2df6-4248-b502-c879a618dc43",
   "metadata": {},
   "source": [
    "### Text Preprocessing Function\n",
    "performs preprocessing on a given text. The preprocessing steps include removing digits, removing punctuation, converting the text to lowercase. The function returns the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de9ffefe-1ae3-431c-8f3a-d0bd7822d554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocesses the text by removing digits, punctuation, converting to lowercase,\n",
    "    removing stopwords, and performing lemmatization.\n",
    "\n",
    "    Args:\n",
    "        text: The text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        The preprocessed text.\n",
    "    \"\"\"\n",
    "    # Remove digits\n",
    "    text = re.sub('\\d', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e4773-a73a-41f6-bba0-da84d90e0bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PDF Files Parsing Function\n",
    "takes a folder path and an optional chunk size as input. It parses all PDF files in the specified folder and splits the extracted text into smaller chunks. The function returns a pandas DataFrame with three columns: filename (the name of the PDF file), text_chunk (a single text chunk extracted from the PDF file), and author (the name of the author, same as the folder name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f84c43-5fdc-42ae-b247-54d1ff0029a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_pdf_files(folder, chunk_size=100):\n",
    "    \"\"\"Parses all PDF files in a given folder and splits the extracted text into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        folder: The path to the folder that contains the PDF files, and it's the name of the author.\n",
    "        chunk_size: The desired size of each text chunk (default: 100 characters).\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the following columns:\n",
    "            * `filename`: The name of the PDF file.\n",
    "            * `text_chunk`: A single text chunk extracted from the PDF file.\n",
    "            * `author`: The name of the author (same as the folder name).\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.pdf'):\n",
    "            with pdfplumber.open(f'{folder}/{file}') as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text = page.extract_text()\n",
    "                    text = preprocess_text(text)\n",
    "\n",
    "                    chunks = split_text(text, chunk_size)\n",
    "\n",
    "                    for chunk in chunks:\n",
    "                        rows.append({\n",
    "                            'filename': file,\n",
    "                            'text_chunk': chunk,\n",
    "                            'author': folder\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ac2c1-4394-4a60-a536-6d46f83670da",
   "metadata": {},
   "source": [
    "### Parsing PDF Files and Saving to CSV\n",
    "The main function is responsible for parsing PDF files of two authors, Racine and Corneille, and saving the extracted data into separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83746fc-9c7d-49d5-82bc-e2a45b4d282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parse_pdf_files('Racine').to_csv(\"data/Racine.csv\", index=False)\n",
    "    parse_pdf_files('Corneille').to_csv(\"data/Corneille.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971a3018-b958-43b0-b18b-e71f7367b112",
   "metadata": {},
   "source": [
    "### Combining and Shuffling the Data\n",
    "combines the data from the CSV files of Corneille and Racine, shuffles the rows randomly, and saves the shuffled data into a new CSV file. This step is crucial for ensuring randomness in the data and avoiding any bias that may arise from the original order of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2913a8a9-6d18-4d55-af3f-00ed807ab109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the CSV files for Corneille and Racine\n",
    "df_corneille = pd.read_csv('data/Corneille.csv')\n",
    "df_racine = pd.read_csv('data/Racine.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df_corneille, df_racine], ignore_index=True)\n",
    "\n",
    "# Shuffle the combined DataFrame\n",
    "shuffled_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Write the shuffled DataFrame to a new CSV file\n",
    "shuffled_df.to_csv('data/shuffled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ac4f1-3426-4c00-a1cb-a16df87c9291",
   "metadata": {},
   "source": [
    "### Loading Shuffled Data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68995366-cc4b-4c1d-b06f-9d728fd3f514",
   "metadata": {},
   "source": [
    "the shuffled data from the \"shuffled.csv\" file is loaded into a Spark DataFrame, enabling further processing and analysis using Spark's distributed computing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b73836-c41c-4b61-831c-1739dab91e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"data/shuffled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f75f2aa7-5299-411a-b73d-2f7cfcef9382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---------+\n",
      "|       filename|          text_chunk|   author|\n",
      "+---------------+--------------------+---------+\n",
      "| MITHRIDATE.pdf|de pompée elle tr...|   Racine|\n",
      "| MORTPOMPEE.pdf|coeur que rome eû...|Corneille|\n",
      "| MORTPOMPEE.pdf|diligence à ses f...|Corneille|\n",
      "|     PHEDRE.pdf|poète quils détes...|   Racine|\n",
      "|PLACEROYALE.pdf|garantir dune dél...|Corneille|\n",
      "| MITHRIDATE.pdf|juge plus sévère ...|   Racine|\n",
      "| ANDROMAQUE.pdf|vainqueur digne d...|   Racine|\n",
      "| MORTPOMPEE.pdf|cette nouvelle vo...|Corneille|\n",
      "|   BERENICE.pdf|seigneur antiochu...|   Racine|\n",
      "| ANDROMAQUE.pdf|forçant de rompre...|   Racine|\n",
      "|     PHEDRE.pdf|dune si belle vie...|   Racine|\n",
      "|   RODOGUNE.pdf|ils ont même sang...|Corneille|\n",
      "| ANDROMAQUE.pdf|madame et puisquo...|   Racine|\n",
      "| MORTPOMPEE.pdf|bien due ils vous...|Corneille|\n",
      "|     PHEDRE.pdf|apprentissage en ...|   Racine|\n",
      "| MITHRIDATE.pdf|cest par de tels ...|   Racine|\n",
      "|     HORACE.pdf|règles infaillibl...|Corneille|\n",
      "|    BAJAZET.pdf|de saintcyr jouée...|   Racine|\n",
      "| ANDROMAQUE.pdf|achille et tout c...|   Racine|\n",
      "|     PHEDRE.pdf|et den faire lélo...|   Racine|\n",
      "+---------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ea48885-32f8-4b10-8fd8-1883ae159021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|          text_chunk|   author|\n",
      "+--------------------+---------+\n",
      "|de pompée elle tr...|   Racine|\n",
      "|coeur que rome eû...|Corneille|\n",
      "|diligence à ses f...|Corneille|\n",
      "|poète quils détes...|   Racine|\n",
      "|garantir dune dél...|Corneille|\n",
      "|juge plus sévère ...|   Racine|\n",
      "|vainqueur digne d...|   Racine|\n",
      "|cette nouvelle vo...|Corneille|\n",
      "|seigneur antiochu...|   Racine|\n",
      "|forçant de rompre...|   Racine|\n",
      "|dune si belle vie...|   Racine|\n",
      "|ils ont même sang...|Corneille|\n",
      "|madame et puisquo...|   Racine|\n",
      "|bien due ils vous...|Corneille|\n",
      "|apprentissage en ...|   Racine|\n",
      "|cest par de tels ...|   Racine|\n",
      "|règles infaillibl...|Corneille|\n",
      "|de saintcyr jouée...|   Racine|\n",
      "|achille et tout c...|   Racine|\n",
      "|et den faire lélo...|   Racine|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"text_chunk\", \"author\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93573b3-878c-4099-91f7-134b7c2c7ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   author|count|\n",
      "+---------+-----+\n",
      "|   Racine| 9299|\n",
      "|Corneille| 9386|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"author\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113df67-8f12-408a-893b-0a19d3c5cf15",
   "metadata": {},
   "source": [
    "### Feature Engineering and Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3158838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assigns a unique numerical label to each distinct author\n",
    "indexer = StringIndexer(inputCol=\"author\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d118b6-1166-4cf5-b4c5-1fc6b0293f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|   author|label|\n",
      "+---------+-----+\n",
      "|Corneille|  0.0|\n",
      "|   Racine|  1.0|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['author','label'].distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090cc44e",
   "metadata": {},
   "source": [
    "the author column is encoded with unique numerical labels using label encoding, which facilitates classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97bbc69-6136-4ffd-bfcb-691977a23012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the text column\n",
    "tokenizer = Tokenizer(inputCol=\"text_chunk\", outputCol=\"words\")\n",
    "\n",
    "# Apply CountVectorizer to get term frequency vectors\n",
    "vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "\n",
    "\n",
    "# Apply IDF to get tf-idf vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce801511-e18a-477f-b1de-5be038fb6359",
   "metadata": {},
   "source": [
    "In the preprocessing step, the text column is tokenized to split the text into individual words. Then, the tokenized words are transformed into term frequency vectors.Next, these term frequency vectors are converted into TF-IDF vectors, assigning weights to words based on their importance in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc948680-9f7a-47b5-85f8-4c8dcdc0e6fc",
   "metadata": {},
   "source": [
    "### Training and Test Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a627b7cc-4130-4524-993c-a0b470d19cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = df.randomSplit((0.6, 0.4), seed=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "885a6431-94f3-4180-a7a4-4d0ff43e5842",
   "metadata": {},
   "source": [
    "### Model Evaluation and Selection with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc1edd0-144a-45c3-be3b-eec6bf514fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression Accuracy: 0.815464057616524\n",
      "RandomForestClassifier Accuracy: 0.6649001222992255\n",
      "Best Model: LogisticRegressionModel\n",
      "Best Accuracy: 0.815464057616524\n"
     ]
    }
   ],
   "source": [
    "# Define the models to be evaluated\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .build()\n",
    "\n",
    "models = [lr, rf]\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Perform K-fold cross-validation and select the best model\n",
    "cv_results = []\n",
    "best_accuracy = 0.0\n",
    "best_model = None\n",
    "\n",
    "for model in models:\n",
    "    pipeline = Pipeline(stages=[tokenizer, vectorizer, idf, model])\n",
    "    \n",
    "    # Create a CrossValidator instance\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5)\n",
    "    \n",
    "    # Run cross-validation\n",
    "    cv_model = crossval.fit(train_data)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    predictions = cv_model.transform(test_data)\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    cv_results.append((model.__class__.__name__, accuracy))\n",
    "    \n",
    "    # Check if the current model is the best one\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = cv_model.bestModel\n",
    "\n",
    "# Print the cross-validation results\n",
    "for result in cv_results:\n",
    "    print(result[0], \"Accuracy:\", result[1])\n",
    "\n",
    "# Use the best model for predictions\n",
    "predictions_best = best_model.transform(test_data)\n",
    "best_model_name = best_model.stages[-1].__class__.__name__\n",
    "best_accuracy = evaluator.evaluate(predictions_best)\n",
    "\n",
    "print(\"Best Model:\", best_model_name)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c09759-4356-4e23-9bbe-2ca7ce379bef",
   "metadata": {},
   "source": [
    "Based on these results, it can be concluded that the Logistic Regression model outperformed the Random Forest Classifier in terms of accuracy for predicting the author of the text. So The Logistic Regression was selected as the best model due to its highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bd693e-f351-456c-a9d7-3f5c0916ab1f",
   "metadata": {},
   "source": [
    " ### Predicting the Author of a new text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a35eac4-5485-4637-96ea-fd24a8239c76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Author is: Racine\n"
     ]
    }
   ],
   "source": [
    "# Sample text to classify\n",
    "text = \"\"\"Hé bien, je l'avouerai, que ma juste colère\n",
    "Aime la guerre autant que la paix vous est chère.\n",
    "235 J'avouerai que brûlant d'une noble chaleur,\n",
    "Je vais contre Alexandre éprouver ma valeur.\n",
    "Du bruit de ses exploits mon âme importunée\n",
    "Attend depuis longtemps cette heureuse journée.\n",
    "Avant qu'il me cherchât, un orgueil inquiet\n",
    "240 M'avait déjà rendu son ennemi secret.\n",
    "Dans le noble transport de cette jalousie,\n",
    "Je le trouvais trop lent à traverser l'Asie.\n",
    "Je l'attirais ici par des voeux si puissants,\n",
    "Que je portais envie au bonheur des Persans.\n",
    "245 Et maintenant encor s'il trompait mon courage,\n",
    "Pour sortir de ces lieux, s'il cherchait un passage,\n",
    "Vous me verriez moi-même armé pour l'arrêter,\n",
    "Lui refuser la paix qu'il nous veut présenter.\"\"\"\n",
    "\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "\n",
    "# Create a DataFrame with a single row containing the preprocessed text\n",
    "data = spark.createDataFrame([(preprocessed_text,)], [\"text_chunk\"])\n",
    "\n",
    "# Make predictions\n",
    "predictions = best_model.transform(data)\n",
    "\n",
    "# Extract predicted author\n",
    "predicted_author = predictions.select(\"prediction\").first()[0]\n",
    "\n",
    "# Map the predicted label to author name\n",
    "author_name = \"Corneille\" if predicted_author == 0 else \"Racine\"\n",
    "\n",
    "# Print the predicted author\n",
    "print(\"The Author is:\", author_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d248965-8a99-4903-8ada-849c1fd6f110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
